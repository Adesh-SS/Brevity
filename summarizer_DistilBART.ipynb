{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601a3b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Combined Chunk Summary:\n",
      " Sentiment analysis is used to classify customer reviews on various online platforms .\n",
      "Toxicity classification is a branch of NLP where the aim is to classify hostile intent .\n",
      "Spam detection is a prevalent binary classification problem in NLP .\n",
      "Machine translation automates translation between different languages .\n",
      "Named entity recognition aims to extract entities in a piece of text into predefined categories . This is the latest in a series of attempts to solve the problems of the world's most serious problems .\n",
      "It is the first attempt at solving the problem of global economic woes .\n",
      "Inventive efforts have been made in the U.S. have failed to reach the same level of success as in the world of economic prosperity .\n",
      "The world's best-selling company is now looking for ways to solve this problem .\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./brevity_small_stage2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./brevity_small_stage2\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def chunk_text(text, chunk_size=512):\n",
    "    \"\"\" Function to split text into chunks smaller than the max token length. \"\"\"\n",
    "    tokens = tokenizer.encode(text, truncation=False)\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def summarize_chunks(chunks):\n",
    "    \"\"\" Function to summarize each chunk and return the combined summary. \"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Decode tokens back into text\n",
    "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        \n",
    "        # Dynamically adjust chunk and summary lengths based on the input size\n",
    "        chunk_size = 512 if len(chunk_text.split()) < 1000 else 1024  # Adjust input chunk size\n",
    "        target_length = 150 if len(chunk_text.split()) < 1000 else 250  # Adjust output summary length\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            chunk_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=chunk_size,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=target_length,  # Dynamically adjusted target length\n",
    "                min_length=target_length // 2,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                length_penalty=1.2,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # Combine summaries\n",
    "    combined_summary = \" \".join(summaries)\n",
    "    return combined_summary\n",
    "\n",
    "# Main code for reading text and summarizing it\n",
    "with open('sample.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Step 1: Split the text into chunks\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "# Step 2: Summarize each chunk\n",
    "final_summary = summarize_chunks(chunks)\n",
    "\n",
    "# Print the final summary\n",
    "print(\"\\n✅ Combined Chunk Summary:\\n\", final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c568274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1068 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Combined Chunk Summary:\n",
      " Climate change is already here, unfolding in real-time across every continent .\n",
      "Many governments still prioritize short-term economic interests over long-term sustainability .\n",
      "Collaboration is the cornerstone of climate action .\n",
      "Technology, regulation, education, and most importantly, collaboration are needed .\n",
      "The power of informed citizens cannot be overstated, says co-authors . Climate justice must be at the heart of every conversation, authors say .\n",
      "They say people least responsible for global emissions often bear the brunt of its consequences .\n",
      "The climate crisis is no longer an environmental issue; it is an everything issue .\n",
      "It affects jobs, migration, public health, geopolitical stability, and intergenerational justice .\n",
      "Author: \"Let us not wait for perfect solutions but commit to imperfect action that paves the way\" The time to act is not tomorrow or next year -- it is now .\n",
      "Let us not divide the world into victims and saviors but recognize our shared fate .\n",
      "\"Let us build not just resilience, but restoration. Let us move forward, informed, united, and unafraid. empowered by the urgency,\" she says .\n",
      "She says: \"Let us stand not just for the planet, but with it\"\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "import torch\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./brevity_small_stage3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./brevity_small_stage3\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def is_similar(s1, s2, threshold = 0.85):\n",
    "    return SequenceMatcher(None, s1.strip(), s2.strip()).ratio() > threshold\n",
    "\n",
    "def filter_redundant(summaries):\n",
    "    filtered = []\n",
    "\n",
    "    for summary in summaries:\n",
    "        if all(not is_similar(summary, existing) for existing in filtered):\n",
    "            filtered.append(summary)\n",
    "    return filtered\n",
    "\n",
    "def chunk_text(text, chunk_size=512, overlap = 50):\n",
    "    \"\"\" Function to split text into chunks smaller than the max token length. \"\"\"\n",
    "    tokens = tokenizer.encode(text, truncation=False)\n",
    "\n",
    "    stride = chunk_size - overlap\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(tokens), stride):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "        # Stop if the chunk is smaller than chunk_size (last part)\n",
    "        if i + chunk_size >= len(tokens):\n",
    "            break\n",
    "    return chunks\n",
    "\n",
    "def summarize_chunks(chunks):\n",
    "    \"\"\" Function to summarize each chunk and return the combined summary. \"\"\"\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Decode tokens back into text\n",
    "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        \n",
    "        # Dynamically adjust chunk and summary lengths based on the input size\n",
    "        chunk_size = 512 if len(chunk_text.split()) < 1000 else 1024\n",
    "        target_length = 150 if len(chunk_text.split()) < 1000 else 250\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            chunk_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=chunk_size,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=target_length,\n",
    "                min_length=target_length // 2,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                length_penalty=1.2,\n",
    "                no_repeat_ngram_size=3,\n",
    "            )\n",
    "\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    summaries = filter_redundant(summaries)\n",
    "\n",
    "    # Combine summaries\n",
    "    combined_summary = \" \".join(summaries)\n",
    "    return combined_summary\n",
    "\n",
    "# Main code for reading text and summarizing it\n",
    "with open('longerVariants.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Step 1: Split the text into chunks\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "# Step 2: Summarize each chunk\n",
    "final_summary = summarize_chunks(chunks)\n",
    "\n",
    "# Print the final summary\n",
    "print(\"\\n✅ Combined Chunk Summary:\\n\", final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brevity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
