{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "601a3b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Combined Chunk Summary:\n",
      " Sentiment analysis is used to classify customer reviews on various online platforms .\n",
      "Toxicity classification is a branch of NLP where the aim is to classify hostile intent .\n",
      "Spam detection is a prevalent binary classification problem in NLP .\n",
      "Machine translation automates translation between different languages .\n",
      "Named entity recognition aims to extract entities in a piece of text into predefined categories . This is the latest in a series of attempts to solve the problems of the world's most serious problems .\n",
      "It is the first attempt at solving the problem of global economic woes .\n",
      "Inventive efforts have been made in the U.S. have failed to reach the same level of success as in the world of economic prosperity .\n",
      "The world's best-selling company is now looking for ways to solve this problem .\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./brevity_small_stage2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./brevity_small_stage2\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def chunk_text(text, chunk_size=512):\n",
    "    \"\"\" Function to split text into chunks smaller than the max token length. \"\"\"\n",
    "    tokens = tokenizer.encode(text, truncation=False)\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def summarize_chunks(chunks):\n",
    "    \"\"\" Function to summarize each chunk and return the combined summary. \"\"\"\n",
    "    summaries = []\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        # Decode tokens back into text\n",
    "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        \n",
    "        # Dynamically adjust chunk and summary lengths based on the input size\n",
    "        chunk_size = 512 if len(chunk_text.split()) < 1000 else 1024  # Adjust input chunk size\n",
    "        target_length = 150 if len(chunk_text.split()) < 1000 else 250  # Adjust output summary length\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            chunk_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=chunk_size,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=target_length,  # Dynamically adjusted target length\n",
    "                min_length=target_length // 2,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                length_penalty=1.2,\n",
    "                no_repeat_ngram_size=3\n",
    "            )\n",
    "\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # Combine summaries\n",
    "    combined_summary = \" \".join(summaries)\n",
    "    return combined_summary\n",
    "\n",
    "# Main code for reading text and summarizing it\n",
    "with open('sample.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Step 1: Split the text into chunks\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "# Step 2: Summarize each chunk\n",
    "final_summary = summarize_chunks(chunks)\n",
    "\n",
    "# Print the final summary\n",
    "print(\"\\n✅ Combined Chunk Summary:\\n\", final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c568274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Combined Chunk Summary:\n",
      " Ukraine has launched a 'large-scale' drone attack on Russian military bombers in Siberia .\n",
      "More than 40 warplanes were reportedly hit, including Tu-95 and Tu-22 strategic bombers .\n",
      "Drones were smuggled into Russia and concealed under roofs of wooden sheds .\n",
      "Ukraine's SBU domestic intelligence agency said it had hit Russian planes worth a combined $7bn (£5.2bn) in the wave of drone strikes . Ukrainian president: 'We had been preparing the operation for more than a year and a half'\n",
      "He said: 'Thirty-four per cent of the strategic cruise missile carriers at the airfields were hit.\n",
      "In total, 117 drones were used in the operation.\n",
      "The office of Ukraine's operation in Russia had been located next to the FSB office .\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./brevity_small_stage3\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./brevity_small_stage3\")\n",
    "\n",
    "# Override generation config to fix early_stopping issue\n",
    "# model.generation_config = GenerationConfig.from_pretrained(\n",
    "#     \"./brevity_small_stage3\",\n",
    "#     early_stopping=True\n",
    "# )\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# print(\"Device:\", device)\n",
    "# print(\"Model loaded successfully.\")\n",
    "\n",
    "def chunk_text(text, chunk_size=512):\n",
    "    \"\"\" Function to split text into chunks smaller than the max token length. \"\"\"\n",
    "    tokens = tokenizer.encode(text, truncation=False)\n",
    "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def summarize_chunks(chunks):\n",
    "    \"\"\" Function to summarize each chunk and return the combined summary. \"\"\"\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        # Decode tokens back into text\n",
    "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        \n",
    "        # Dynamically adjust chunk and summary lengths based on the input size\n",
    "        chunk_size = 512 if len(chunk_text.split()) < 1000 else 1024\n",
    "        target_length = 150 if len(chunk_text.split()) < 1000 else 250\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            chunk_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=chunk_size,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=target_length,\n",
    "                min_length=target_length // 2,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                length_penalty=1.2,\n",
    "                no_repeat_ngram_size=3,\n",
    "            )\n",
    "\n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    # Combine summaries\n",
    "    combined_summary = \" \".join(summaries)\n",
    "    return combined_summary\n",
    "\n",
    "# Main code for reading text and summarizing it\n",
    "with open('sample.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Step 1: Split the text into chunks\n",
    "chunks = chunk_text(text)\n",
    "\n",
    "# Step 2: Summarize each chunk\n",
    "final_summary = summarize_chunks(chunks)\n",
    "\n",
    "# Print the final summary\n",
    "print(\"\\n✅ Combined Chunk Summary:\\n\", final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brevity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
